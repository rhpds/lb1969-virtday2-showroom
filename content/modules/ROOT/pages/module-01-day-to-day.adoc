= Enabling Basic Day to Day Operations

== Creating a Dynamic Inventory for OpenShift Virtual Machines

Dynamic inventories allow Ansible Automation Platform (AAP) to automatically
fetch and update the inventory of systems from external sources, eliminating
the need for manual inventory management.

In this lab, we will configure a dynamic inventory to pull data from OpenShift
Virtualization. This will enable AAP to manage OpenShift VMs residing in the
`vms-aap-day2` namespace of an OCP cluster.

== Log in to the OpenShift Environment

. Log in to your OpenShift environment using the following details:

* Console URL: link:{openshift_cluster_console_url}[{openshift_cluster_console_url}, window="_blank"]
* Username: `{openshift_cluster_admin_username}`
* Password: `{openshift_cluster_admin_password}`

== Ansible Automation Platform Environment Details:

. Log in to your AAP environment using the following details:

* AAP Dashboard: link:{aap_controller_web_url}[{aap_controller_web_url}, window="_blank"]
* Username: `{aap_controller_admin_user}`
* Password: `{aap_controller_admin_password}`

image::aap_login.png[title="AAP Login", link=self, window=blank]

=== Create an Inventory

. Navigate to *Automation Execution → Infrastructure → Inventories*.
+
. Click the *Create inventory* drop-down box and select the *Create inventory* option.
+
. In the *Create Inventory* form:
   * Provide a name: `OpenShift Virtual Machines`.
   * Select `Default` as the Organization.
+
. Click the *Create inventory* button.
+
image::create_inventory1.png[title="Create Inventory", link=self, window=blank]

=== Add a Source to the Inventory

. After creating the inventory, switch to the *Sources* tab.
+
image::sources.png[title="Sources", link=self, window=blank]
+
. Select the *Create source* button.
+
. In the *Create Source* form:
   * Provide a name: `OpenShift Virtual Machines Source`.
   * Choose `Day 2 EE` as the execution environment.
   * Select `OpenShift Virtualization` as the *Source* type.
   * Add the `OpenShift Credential` (Bearer Token was pre-created)
   * Select `Update on Launch` checkbox within Options.
   * Set `Cache timeout` to `0` seconds
   * Set `Source variables` to look for VMs only within the `vms-aap-day2` namespace
+
```
namespaces:
  - vms-aap-day2
```
+
. Click the *Create source* button to save the configuration.
+
image::create_inventory_source.png[title="Create Inventory Source", link=self, window=blank]

===  Update the Inventory
. Launch the inventory update:
   * Click the *Launch Inventory Update* button in the top-right corner.
+
image::update_inventory.png[title="Update Inventory", link=self, window=blank]
+
. Wait for the *Last Job Status* to show `Success`.

=== Verify the Inventory
. Once the update completes successfully:
   * Click the *Back to Inventory Sources* button.
   * Switch to the *Hosts* tab.
+
. Confirm that the Virtual Machines from the `vms-app-day2` namespace of your OpenShift cluster are listed as inventory hosts.
+
image::verify_hosts.png[title="Verify Hosts", link=self, window=blank]

=== Test Connectivity of the Virtual Machines

Ensure that you can ping those VMs
within the *AAP* dashboard {aap_controller_web_url} as follows:

. Login to the AAP Dashboard using the credentials provided in the `Setup` step.
+
. Navigate to Automation Execution -> Infrastructure -> Inventories
   * Select the `OpenShift Virtual Machines` Inventory
   * Within the Details page of the `OpenShift Virtual Machines` Inventory, select the *Hosts* tab.
+
. Select the 3 VMs and click the *Run command* button
   * Within the Run command Details, select the `ping` module and click *Next*.
   * Within the Run command Execution Environment, Select the *Day2 EE* execution environment and click *Next*.
   * Within the Run command Credential, select the pre-created *Workshop Credential* and click *Next*.
   * Within the Run command Review, select *Finish*.
+
. Verify Results: You should see a list of VMs along with their statuses similar to:
+
----
vms-aap-day2-rhel9-vm1 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
vms-aap-day2-rhel9-vm2 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
vms-aap-day2-rhel9-vm3 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
----

. You may also confirm the VMs are running using the *OpenShift UI* dashboard {openshift_cluster_console_url}:

*Virtualization -> VirtualMachines* within the `vms-aap-day2` project.

image::vms-ocp-dashboard.png[title='Virtual Machines Running on OpenShift', link=self, window=blank]

== Security and Compliance [compliance operator]

=== Getting started

. To set up a security scan, navigate to the Operators tab and select Installed Operators, then select the Compliance Operator.
+
image::module-01-day-to-day/compliance_operator.png[link=self, window=blank, width=100%]
+
. This takes you to the operator details page, from here move to the ScanSetting tab
+
image::module-01-day-to-day/compliance_details.png[link=self, window=blank, width=100%]
+
. Select Create ScanSetting
+
image::module-01-day-to-day/create_scansetting.png[link=self, window=blank, width=100%]
+
. In the ScanSetting yaml details, note the 'autoApplyRemediations' =False, the roles section includes both master & worker nodes,
and the name can be set according to your choosing.
+
image::module-01-day-to-day/scansetting_details.png[link=self, window=blank, width=100%]
+
. Before moving on, there are quite a number of predefined profiles that can be used for scanning purposes on the 'Profiles' tab.
We’ll use the fedramp moderate profile = rhcos4-moderate
+
image::module-01-day-to-day/profiles_detail.png[link=self, window=blank, width=100%]
+
- Additional detail on these profiles can be found here-
https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/security_and_compliance/compliance-operator#compliance-operator-supported-profiles
- The next step is to set up a ScanSettingBinding, navigate to that tab and select create ScanSettingBinding
+
image::module-01-day-to-day/create_scansettingbinding.png[link=self, window=blank, width=100%]
+
. In the ScanSettingBinding yaml details, the name (fedramp1) can be set to whatever you like.
Note the profile is set to rhcos4-moderate (the fedramp moderate profile), and the ScanSetting is set to the previously defined ScanSetting (scan1).
+
image::module-01-day-to-day/scansettingbinding_details.png[link=self, window=blank, width=100%]
+
. After the ScanSettingBinding is created this will kick of the fedramp1 Compliance Suite
+
image::module-01-day-to-day/compliance_suite.png[link=self, window=blank, width=100%]
+
. This Compliance Suite (fedramp1) kicks off the defined profile (rhcos4-moderate) scans for each of the defined roles in the ScanSetting (master/worker)
+
image::module-01-day-to-day/compliance_scan.png[link=self, window=blank, width=100%]
+
. The scan takes ~3-4 minutes to complete, then you can check the failures on the ComplianceCheckResult tab.
+
image::module-01-day-to-day/checkcomplianceresults.png[link=self, window=blank, width=100%]

== Configure Network Policies to manage VM Traffic

=== Introduction

To demo network policy, we have a standard VM.
The user logs in via console, can ping google or other public IP by default.
Show how we can set a network policy to block egress of that vm, or limit egress to only other VMs in the same namespace.

==== See that the VM can ping google

. Click VirtualMachines, click *rhel9-vm1* click Console.
If you do not see any Virtual Machines make sure you are in vms-aap-day2 project.
+
image::module-01-day-to-day/view_vm.png[link=self, window=blank, width=100%]
+
. Click on the Console tab and using the provided login perform a ping test to google.com.
+
image::module-01-day-to-day/login_vm.png[link=self, window=blank, width=100%]
+
NOTE: There is a Copy to clipboard button and a Paste button available here, which makes the login process much easier.
+
. Once you are logged in, execute the following command:
.. `ping google.com`
+
image::module-01-day-to-day/ping_site.png[link=self, window=blank, width=100%]
+
. Press *Control+C* to stop the ping.
+
. From the OpenShift console, Click on Workloads > Pods. Click on the pod name for *rhel9-vm1*.
+
image::module-01-day-to-day/select_pod.png[link=self, window=blank, width=100%]
+
. In the Pod details section click Edit.
+
image::module-01-day-to-day/pod_details.png[link=self, window=blank, width=100%]
+
. Add app=network-policy-deny to the labels and click Save.
+
image::module-01-day-to-day/pod_label.png[link=self, window=blank, width=100%]
+
. Repeat this process for *rhel9-vm2*.

==== Create the Network Policy

. Under Networking click on *NetworkPolicies* then *Create NetworkPolicy*.
+
image::module-01-day-to-day/networkpolicy.png[link=self, window=blank, width=100%]
+
. In NetworkPolicies fill out these fields and Click *Save*.

.. Policy name: ping-egress-deny
.. Key: app
.. Value: network-policy-deny
.. Check Deny all egress traffic
+
image::module-01-day-to-day/network_policy_configure.png[link=self, window=blank, width=100%]
+
. You can verify what pods are affected pods by the network policy.
+
image::module-01-day-to-day/affected_pod.png[link=self, window=blank, width=100%]

==== See that the VM CANNOT ping google

. Go back to Virtualization > *Virtual Machines* and click *rhel9-vm1*.
.. Click on the *Console* tab and login to the VM
.. Enter the command ping google.com. You should get no responce.
+
image::module-01-day-to-day/ping_site_deny.png[link=self, window=blank, width=100%]
+
. Once complete, delete the network policy you created.

== Enable and Explore Alerts, Graphs, and Logs

==== Node Graphs

Now lets look at the metrics for the Nodes.

. Under *Compute* Click on *Nodes*. From This view you can see the Node status and role.
+
image::module-01-day-to-day/node_list.png[link=self, window=blank, width=100%]
+
. Click on one of the nodes.
+
image::module-01-day-to-day/node_example.png[link=self, window=blank, width=100%]
+
. You will see the Utilization. You can select if you want to see 1, 6 or 24 hours.

==== Virtal Machine Graphs

. Click on *Workloads* > *Deployments* > *loadmaker*. Ensure you are in Project: *windows-vms*.
+
image::module-01-day-to-day/select_loadmaker.png[link=self, window=blank, width=100%]
+
. You should see 1 Pod under the Deployment Details tab.
+
image::module-01-day-to-day/show_num_pod.png[link=self, window=blank, width=100%]
+
. Click on *Environment* tab
+
image::module-01-day-to-day/lm_pod_config.png[link=self, window=blank, width=100%]
+
. In *Virtualization* > *VirtualMachine* Select *windows-vms* project to show *winweb01*, *winweb02* and *database*. You should only see *database* and *winweb01* powered on. Power on them if needed.

. Click on *winweb01*
+
image::module-01-day-to-day/ob_select_vm.png[link=self, window=blank, width=100%]
+
. On the main page of the VM there is a utilization section that show the basic status of the VM updated every 15 seconds.
- Each of the line graphs are clickable.
- Note: CPU overcommute in the environment is 10:1 overcommitment which is the default setting.

. You can adjust the time range going from 5 min to 1 week.
+
image::module-01-day-to-day/ob_select_vm.png[link=self, window=blank, width=100%]
+
. By the Network Transfer and Clicking Breakdown by network section you can see how much traffic is passing through in this space it only shows default because there is only one network adapter.
+
image::module-01-day-to-day/select_network.png[link=self, window=blank, width=100%]
+
. Click on CPU Usage bar graph
+
image::module-01-day-to-day/select_cpu.png[link=self, window=blank, width=100%]
+
. In the Metrics you see how the CPU spiked. here you can see over the last 30 minutes
+
image::module-01-day-to-day/cpu_metrics.png[link=self, window=blank, width=100%]
+
. You can change the interval time anywhere from 5 minutes to 2 weeks.
+
image::module-01-day-to-day/change_interval.png[link=self, window=blank, width=100%]
+
. Here you can change the refresh timing.
+
image::module-01-day-to-day/change_refresh.png[link=self, window=blank, width=100%]
+
. You can also add queries
+
image::module-01-day-to-day/select_qurey.png[link=self, window=blank, width=100%]
+
. Lets add a query "sum(rate(kubevirt_vmi_vcpu_wait_seconds_total{name='winweb01',namespace='windows-vms'}[5m])) BY (name, namespace)" without the "". This query will show the amouont of time spent by each vcpu while waiting on I/O.
+
image::module-01-day-to-day/example_query.png[link=self, window=blank, width=100%]

==== Dashboards

. Click on *Observe* > *Dashboard*.
+
image::module-01-day-to-day/dashboard.png[link=self, window=blank, width=100%]
+
. Click on *API Performance* and search for *KubeVirt/Infrastructure Resources/Top Consumers*
+
image::module-01-day-to-day/kubevirt_dashboard.png[link=self, window=blank, width=100%]
+
. This dashboard you will see the top consumer for your virtual machines. Let dive in and look at the Top Consumers of CPU by virt-launcher Pods and Click *Inspect*
+
image::module-01-day-to-day/cpu_inspect.png.png[link=self, window=blank, width=100%]
+
. You can can select the VMs you want to see in the graph.
+
image::module-01-day-to-day/metrics_select.png[link=self, window=blank, width=100%]
+

== VM Management (Stop, Start, Restart Guest VMs)

In this section, you'll learn how to manage the lifecycle of your guest VMs
running in Red Hat OpenShift Virtualization using Ansible Automation
Platform (AAP). While much of the groundwork such as creating playbooks and
VM task files has already been completed for you, this lab focuses on
understanding how the pieces work together and how to run the automation via
AAP.

To start, you’ll perform common VM lifecycle actions such as stopping, starting, and
restarting all VMs in a given namespace, and you’ll gain insight into how the
automation behind these actions is structured.

=== The existing setup

To assist with your experience, the following content has already been created and configured:

* The `tasks/main.yml` file has been pre-populated with dynamic task inclusion logic.
* The Ansible playbook (`manage_vm_playbook.yml`) that calls the appropriate task based on input variables is in place.
* Task files for stopping, starting, and restarting VMs (`stop_vm.yml`, `start_vm.yml`, and `restart_vm.yml`) have been pre-written.

Although you don’t need to create or modify these files, it's important to understand how they work, as you’ll be referencing them when creating job templates in AAP.


=== Understanding the task files

Each of the task files works by retrieving all virtual machines within a
specific namespace (in our case `vms-aap-day2`)  and then performing an action
(stop, start, restart) based on their current status. The
`ansibel.builtin.debug` task provides insights to understanding the structure of
your VM resource `vm_info` to identify key fields required to create dynamic
Ansible tasks.

=== `stop_vm.yml`

This task file stops any VMs that are currently running within a given namespace.

```
---
- name: Get all VirtualMachines in the namespace
  redhat.openshift_virtualization.kubevirt_vm_info:
    namespace: "{{ vm_namespace }}"
  register: vm_info

- name: Debug the vm_info variable
  ansible.builtin.debug:
    var: vm_info

- name: Stop VM using OpenShift API
  ansible.builtin.uri:
    url: "{{ OCP_HOST }}/apis/subresources.kubevirt.io/v1/namespaces/{{ vm_namespace }}/virtualmachines/{{ item.metadata.name }}/stop"
    method: PUT
    headers:
      Authorization: "Bearer {{ OCP_BEARER_TOKEN }}"
    validate_certs: false
    status_code:
      - 202
  loop: "{{ vm_info.resources }}"
  loop_control:
    label: "{{ item.metadata.name }}"
  when: item.status.printableStatus != "Stopped"

```

=== `start_vm.yml`

This task file starts any VMs that are currently stopped within a given
namespace.

```
---
- name: Get all VirtualMachines in the namespace
  redhat.openshift_virtualization.kubevirt_vm_info:
    namespace: "{{ vm_namespace }}"
  register: vm_info

- name: Debug the vm_info variable
  ansible.builtin.debug:
    var: vm_info

- name: Start VM using OpenShift API
  ansible.builtin.uri:
    url: "{{ OCP_HOST }}/apis/subresources.kubevirt.io/v1/namespaces/{{ vm_namespace }}/virtualmachines/{{ item.metadata.name }}/start"
    method: PUT
    headers:
      Authorization: "Bearer {{ OCP_BEARER_TOKEN }}"
    validate_certs: false
    status_code:
      - 202
  loop: "{{ vm_info.resources }}"
  loop_control:
    label: "{{ item.metadata.name }}"
  when: item.status.printableStatus != "Running"
```

=== `restart_vm.yml`

This task file restarts  any VMs that are currently running within a given
namespace.

```
---
- name: Get all VirtualMachines in the namespace
  redhat.openshift_virtualization.kubevirt_vm_info:
    namespace: "{{ vm_namespace }}"
  register: vm_info

- name: Debug the vm_info variable
  ansible.builtin.debug:
    var: vm_info

- name: Restart VM using OpenShift API
  ansible.builtin.uri:
    url: "{{ OCP_HOST }}/apis/subresources.kubevirt.io/v1/namespaces/{{ vm_namespace }}/virtualmachines/{{ item.metadata.name }}/restart"
    method: PUT
    headers:
      Authorization: "Bearer {{ OCP_BEARER_TOKEN }}"
    validate_certs: false
    status_code:
      - 202
  loop: "{{ vm_info.resources }}"
  loop_control:
    label: "{{ item.metadata.name }}"
  when: item.status.printableStatus != "Running"
```

These task files use the OpenShift REST API directly using the
`ansible.builtin.uri` module to invoke the appropriate lifecycle action of
stopping, starting or restarting.

The debug task helps you visualize the
structure of VM data returned by the `kubevirt_vm_info` module and breaks down
as follows:


* The `kubevirt_vm_info` module retrieves all VMs in the namespace.

* `metadata.name`: The name of the VirtualMachine.

* `metadata.namespace`: The namespace the VM belongs to.

* The `loop_control` option sets a label for each task iteration, showing the VM name (`item.metadata.name`) in the output. This makes the playbook output more readable and easier to debug.

* `status.printableStatus`: The current status of the VM (e.g., Stop, Start,Restart).

A snippet sample of the `ansible.builtin.debug module is shown below.

```
changed: true
result:
  apiVersion: kubevirt.io/v1
  kind: VirtualMachine
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: >
        ...
    ...
    name: rhel9-vm1
    namespace: vms-aap-day2
  spec:
    ...
  status:
    ...
    printableStatus: Stopped
  ...
```

=== Creating & Running the VM Job Templates with Ansible Automation Platform

Each VM lifecycle template that you will create takes advantage of the
`manage_vm_playbook.yml`. In this section,  you will head to the Ansible
Automation Platform (AAP) dashboard and create a VM Job Template for each
scenario: Start VMs, Stop VMs, Restart VMs.

The following instructions showcase how to create the Stop VMs Template within
AAP.

. Within the AAP UI dashboard, navigate to *Automation Execution → Templates*, click the *Create template* button, and chose *Create job template*.
+
. Fill out the following details within the job template
+
|===
| Parameter         | Value

| Name          | Stop VMs
| Job Type      | Run
| Inventory     | OpenShift Virtual Machines
| Project       | Workshop Project
| Playbook      | `solutions/manage_vm_playbook.yml`
| Execution Environment | Day2 EE
| Credentials   | OpenShift Credential
| Extra variables            | `vm_namespace: vms-aap-day2` +
                               `task_file: stop_vm.yml`
|===
+
. Click *Create job template*.
+
. Once the `Stop VMs` Job Template is created, select the *Launch template* button on the top right corner to run the job.
+
. Head to the OpenShift UI dashboard, you can verify the VMs are running within the *Virtualization -> Virtual Machines* section.

Repeat this process to create the *Start VMs* and *Restart VMs* Ansible Job
Templates. The details for each is provided below.

For Starting VMs, use the following details to create the job template

|===
| Parameter         | Value

| Name          | Start VMs
| Job Type      | Run
| Inventory     | OpenShift Virtual Machines
| Project       | Workshop Project
| Playbook      | `solutions/manage_vm_playbook.yml`
| Execution Environment | Day2 EE
| Credentials   | OpenShift Credential
| Extra variables            | `vm_namespace: vms-aap-day2` +
                               `task_file: start_vm.yml`
|===

For Restarting VMs, use the following details to create the job template

|===
| Parameter         | Value

| *Name*          | Restart VMs
| *Job Type*      | Run
| *Inventory*     | OpenShift Virtual Machines
| *Project*       | Workshop Project
| *Playbook*      | `solutions/manage_vm_playbook.yml`
| *Execution Environment* | Day2 EE
| *Credentials*   | OpenShift Credential
| Extra variables            | `vm_namespace: vms-aap-day2` +
                               `task_file: restart_vm.yml`
|===

Once you have created these Job templates select the *Launch template* button
on the top right corner to run the job and notice the changes of these VMs
within the OpenShift console.

== VM Management (Patching your VMs)

In this exercise, you’ll automate the patching of RHEL virtual machines by applying
only security-related updates using Ansible Automation Platform.

The virtual machines you'll target are already part of a dynamic inventory that
was set up in a previous step — specifically, the *OpenShift Virtual Machines*
inventory.

Rather than writing playbooks or tasks from scratch, you'll be working with
provided automation content, which includes:

* A task file that performs security updates using the `dnf` module.
* A playbook that executes roles responsible for system registration and patching.

Your goal is to understand what this content does and then create a Job Template
to execute the automation using Ansible Automation Platform’s web UI.

[NOTE]
====
This lab uses Vault Credentials to securely handle sensitive
authentication data and a subscription management role to register RHEL systems
to Red Hat.  This ensures VMs have access to the correct repositories for
updates and demonstrates secure automation practices.
====

=== Understanding the Provided Task File: `update_security_packages.yml`

This task file lives inside the `tasks/` directory of the
`redhatone.vm_management.vm_management` role. It uses the `ansible.builtin.dnf`
module to scan for and install the latest security-related updates on all hosts
in the inventory.

[source,yaml]
----
- name: Update security-related packages on all hosts
  ansible.builtin.dnf:
    name: "*"
    security: true
    state: latest
----

Breakdown:

* `name: "*"` — Targets all available packages.
* `security: true` — Filters for only security-related updates.
* `state: latest` — Ensures the latest available security updates are installed.

This task is designed to be modular. It’s included in your role and can be
triggered from any playbook using a variable like `task_file`, which you’ll see
used shortly.

== Understanding the Provided Playbook: `patch_vm_playbook.yml`

This playbook is responsible for executing the logic that handles both system
registration and patching. It is already present in your project directory.

[source,yaml]
----
- name: Patch Virtual Machines
  hosts: all
  roles:
    - redhatone.vm_management.rhsm_subscription
    - redhatone.vm_management.vm_management
----

Breakdown:

* `redhatone.vm_management.rhsm_subscription`: Registers the RHEL VMs to Red Hat using credentials provided via Vault. This step ensures the systems have access to the necessary repositories for receiving updates.
* `redhatone.vm_management.vm_management`: Calls the role that includes the security update task (`update_security_packages.yml`), referenced via the `task_file` variable.

The playbook ensures that every target host goes through both registration and
patching in the correct order.

=== Creating the Patch VMs Job Template in Ansible Automation Platform

You’ll now connect all the pieces through the AAP web interface and run the
automation using a Job Template.

. Navigate to *Automation Execution → Templates*.
. Click *Create template → Create job template*.
. Fill out the following fields:
+
|===
| Parameter               | Value

| Name                    | Patch VMs
| Job Type                | Run
| Inventory               | OpenShift Virtual Machines
| Project                 | Workshop Project
| Playbook                | `solutions/patch_vm_playbook.yml`
| Execution Environment   | Day2 EE
| Credentials             | Workshop Credential, Vault Credential
| Extra Variables         | `task_file: update_security_packages.yml`
| Privilege Escalation    | Enabled

|===
+
WARNING: Notice there are two credentials attached and privilege escalation is enabled.
+
. Click *Create job template*.
. Once created, click *Launch Template* (top-right) to start the job.
. Once the *Patch VMs* Job is complete, you should see output similar to:
+
image::patch_vm.png[title='Patch VM', link=self, window=blank]

=== Reviewing the Job Output

After the job runs, you’ll be able to see:

* A task-by-task breakdown showing which operations were performed.
* Output for the task titled `Update security-related packages on all hosts`.
* Per-host details indicating which security updates were applied.

These views help validate that the automation worked and that each VM received
the expected patches.

image::patch_vm_task.png[title='Detail Task View', link=self, window=blank]

image::patch_vm_host_details.png[title='Host Details', link=self, window=blank]

== VM Management (Hot-Plugging CPU and Memory Resources)

In this section, you will learn how to hot-plug CPU and memory resources into a
running Virtual Machine (VM) using Ansible Automation Platform and the
`redhat.openshift_virtualization` collection.

Hot-plugging is the ability to add or remove hardware resources, such as CPU or
memory, to a running VM without requiring a reboot. This capability is critical
for dynamic workloads, allowing you to scale resources based on demand while
minimizing downtime.

This exercise focuses on using *instance types*, which are reusable objects in
OpenShift Virtualization that define the resources and characteristics for VMs.
Instance types simplify resource management by enabling consistent
configurations across VMs.

=== What Are Instance Types?

An instance type is a reusable configuration object where you define resources
(like CPU and memory) and characteristics for new VMs. OpenShift Virtualization
provides two types of instance types:

. *VirtualMachineInstancetype*: A namespaced object for instance types limited to a specific namespace.
. *VirtualMachineClusterInstancetype*: A cluster-wide object for instance types available across all namespaces.

Both types share the same `VirtualMachineInstancetypeSpec`, allowing you to
define custom configurations or use the variety of instance types included by
default when OpenShift Virtualization is installed.

By using instance types, you can simplify VM configuration management and ensure
consistency, making them the *recommended approach* for hot-plugging resources.

In this lab, you will primarily focus on using the instance type method while
also learning about the classic approach of directly modifying the VM
specification for context.

NOTE: The classic method only works when creating VMs that do not use an instance type.

=== How to Identify if a VM Uses Instance Types or Not?

To determine whether a VM is created with an instance type or not, follow these steps:

. Navigate to the *Overview* tab of the VM (e.g., `rhel9-vm1`) in the OpenShift Virtualization dashboard.
. In the *Details* section, look for the following:
   - *Instance Type*: If the VM uses an instance type, this field will display the name of the instance type applied to the VM (e.g., `u1.small`).
   - *Template*: If no instance type is used, this field will display either `None` or the name of the template used to create the VM.

You can use these visual cues to identify whether the VM relies on an instance
type or a traditional template.

Below is an example image illustrating both scenarios:

* One VM shows an assigned instance type.
+
* Another VM indicates `rhel9-server-small` under the template field, indicating that the `rhel9-server-small` template was used.
+
image::example_instance_type_and_template.png[title="Instance Type vs. Template", link=self, window=blank]


=== Using the pre-created `hot_plug.yml` to update resources

The `hot_plug.yml` file consists of a task that updates a running VM by applying a
new instance type. This approach lets you add CPU and memory resources
dynamically without needing to recreate or power off the VM.

The `instance_type` method is the recommended approach for hot-plugging
resources into a VM. It ensures consistent and reusable resource configurations
across multiple VMs while leveraging the powerful features of OpenShift
Virtualization.

----
- name: Swap Instance Type to add more Resources
  redhat.openshift_virtualization.kubevirt_vm:
    name: "rhel9-vm1"
    namespace: "{{ vm_namespace }}"
    state: present
    run_strategy: Always
    instancetype:
      name: "{{ instance_type }}"
      revisionName: ""
----

*Explanation of the Task:*

- *redhat.openshift_virtualization.kubevirt_vm*: Specifies the module used to manage VMs in OpenShift Virtualization.
- *name*: The name of the VM to which the new resources will be applied.
- *namespace*: The namespace in which the VM resides.
- *state*: Ensures the VM is present and available.
- *instancetype*: Defines the instance type for the VM, allowing you to use pre-configured or custom resource settings.
  - *name*: The name of the instance type to be applied.
  - *revisionName*: Optionally specifies the exact revision of the instance type, ensuring compatibility with the VM. Typically auto-generated, thus left empty.

WARNING: VMs must be created using Instance Types for this task method to work. Otherwise use the Classic method.

=== Classic Method: Modifying the Spec Directly (Informational Only)

The classic method involves directly modifying the VM's `spec` file to update
CPU and memory resources. While this approach is flexible, it lacks the
reusability and consistency offered by instance types, making it less ideal for
managing resources across multiple VMs.


[source, yaml]
----
- name: Modify CPU & Memory Resources
  redhat.openshift_virtualization.kubevirt_vm:
    name: "rhel9-vm2"
    namespace: "{{ vm_namespace }}"
    state: present
    spec:
      domain:
        cpu:
          sockets: 2
        memory:
          guest: 4Gi
----

*Explanation of the Task:*

- *redhat.openshift_virtualization.kubevirt_vm*: Specifies the module used to manage VMs in OpenShift Virtualization.
- *name*: The name of the VM being modified.
- *namespace*: The namespace in which the VM resides.
- *state*: Ensures the VM is in the desired state, in this case, `present`.
- *spec*: Directly modifies the VM's specification.
  - *domain*: Contains settings related to the VM's virtualized environment.
    - *cpu*: Specifies the number of CPU sockets for the VM (e.g., `2`).
    - *memory*: Defines the memory allocated to the VM, (e.g., `4Gi`).

WARNING: Classic VMs are not part of this lab exercise and the Classic Method is for informational purposes only.

=== Create and Run the Hot-Plug Job Template

. Within the AAP UI Dashboard, navigate to *Automation Execution → Templates*.
. Click *Create Template* and select *Create job template*.
. Fill in the following details:
+
[cols="2,3",options="header"]
|===
| Parameter | Value
| *Name* | Hot Plug VMs
| *Job Type* | Run
| *Inventory* | OpenShift Virtual Machines
| *Project* | Workshop Project
| *Playbook* | `solutions/manage_vm_playbook.yml`
| *Execution Environment* | Day 2 EE
| *Credentials* | OpenShift Credential
| *Extra variables* | `vm_namespace: vms-aap-day2` +
                      `task_file: hot_plug.yml` +
                      `instance_type: u1.2xmedium`
|===
+
. Click *Create Job Template*.
. Launch the job by selecting *Launch Template* from the top-right corner.
. When the job completes, head to the OpenShift UI dashboard and view the details of the `rhel9-vm1` Virtual Machine. You should see that the new size `u1.2xmedium` is now being used.
