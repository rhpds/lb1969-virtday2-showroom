= Enabling Basic Day to Day Operations

== Creating a Dynamic Inventory for OpenShift Virtual Machines

Dynamic inventories allow Ansible Automation Platform (AAP) to automatically
fetch and update the inventory of systems from external sources, eliminating
the need for manual inventory management. 

In this lab, we will configure a dynamic inventory to pull data from OpenShift
Virtualization. This will enable AAP to manage OpenShift VMs residing in the
`vms-aap-day2` namespace of an OCP cluster.

=== Create an Inventory

. Navigate to **Automation Execution → Infrastructure → Inventories**.
+
. Select the **Create inventory** button.
+
. In the **Create Inventory** form:
   * Provide a name: `OpenShift Virtual Machines`.
   * Select `Default` as the Organization.
+
. Click the **Create inventory** button.
+
image::create_inventory.png[title="Create Inventory", link=self, window=blank]

=== Add a Source to the Inventory

. After creating the inventory, switch to the **Sources** tab.
+
image::sources.png[title="Sources", link=self, window=blank]
+
. Select the **Create source** button.
+
. In the **Create Source** form:
   * Provide a name: `OpenShift Virtual Machines Source`.
   * Choose `Day 2 EE` as the execution environment.
   * Select `OpenShift Virtualization` as the **Source** type.
   * Add the `OpenShift Credential` (Bearer Token was pre-created)
   * Select `Update on Launch` checkbox within Options.
   * Set `Cache timeout` to `0` seconds
   * Set `Source variables` to look for VMs only within the `vms-aap-day2` namespace
+
```
namespaces:
  - vms-aap-day2
```
+
. Click the **Create source** button to save the configuration.
+
image::create_inventory_source.png[title="Create Inventory Source", link=self, window=blank]

===  Update the Inventory
. Launch the inventory update:
   * Click the **Launch Inventory Update** button in the top-right corner.
+
image::update_inventory.png[title="Update Inventory", link=self, window=blank]
+
. Wait for the **Last Job Status** to show `Success`.

=== Verify the Inventory
. Once the update completes successfully:
   * Click the **Back to Inventory Sources** button.
   * Switch to the **Hosts** tab.
+
. Confirm that the Virtual Machines from the `vms-app-day2` namespace of your OpenShift cluster are listed as inventory hosts.
+
image::verify_hosts.png[title="Verify Hosts", link=self, window=blank]

=== Test Connectivity of the Virtual Machines

Ensure that you can ping those VMs
within the **AAP** dashboard {aap_controller_web_url} as follows:

. Login to the AAP Dashboard using the credentials provided in the `Setup` step.
+
. Navigate to Automation Execution -> Infrastructure -> Inventories
   * Select the `OpenShift Virtual Machines` Inventory
   * Within the Details page of the `OpenShift Virtual Machines` Inventory, select the *Hosts* tab.
+
. Select the 3 VMs and click the *Run command* button
   * Within the Run command Details, select the `ping` module and click *Next*.
   * Within the Run command Execution Environment, Select the *Day2 EE* execution environment and click *Next*.
   * Within the Run command Credential, select the pre-created *Workshop Credential* and click *Next*.
   * Within the Run command Review, select *Finish*.
+
. Verify Results: You should see a list of VMs along with their statuses similar to:
+
----
vms-aap-day2-rhel9-vm1 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
vms-aap-day2-rhel9-vm2 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
vms-aap-day2-rhel9-vm3 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
----

. You may also confirm the VMs are running using the **OpenShift UI** dashboard {openshift_cluster_console_url}:

**Virtualization -> VirtualMachines** within the `vms-aap-day2` project.

image::vms-ocp-dashboard.png[title='Virtual Machines Running on OpenShift', link=self, window=blank]

== Security and Compliance [compliance operator]

=== Getting started

. To set up a security scan, navigate to the Operators tab and select Installed Operators, then select the Compliance Operator.
+
image::day-to-day/compliance_operator.png[link=self, window=blank, width=100%]
+
. This takes you to the operator details page, from here move to the ScanSetting tab
+
image::day-to-day/compliance_details.png[link=self, window=blank, width=100%]
+
. Select Create ScanSetting
+
image::day-to-day/create_scansetting.png[link=self, window=blank, width=100%]
+
. In the ScanSetting yaml details, note the 'autoApplyRemediations' =False, the roles section includes both master & worker nodes,
and the name can be set according to your choosing.
+
image:day-to-day/scansetting_details.png[link=self, window=blank, width=100%]
+
. Before moving on, there are quite a number of predefined profiles that can be used for scanning purposes on the 'Profiles' tab. 
We’ll use the fedramp moderate profile = rhcos4-moderate
+
image::day-to-day/profiles_detail.png[link=self, window=blank, width=100%]
+
- Additional detail on these profiles can be found here- 
https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/security_and_compliance/compliance-operator#compliance-operator-supported-profiles
- The next step is to set up a ScanSettingBinding, navigate to that tab and select create ScanSettingBinding
+
image:day-to-day/create_scansettingbinding.png[link=self, window=blank, width=100%]
+
. In the ScanSettingBinding yaml details, the name (fedramp1) can be set to whatever you like.  
Note the profile is set to rhcos4-moderate (the fedramp moderate profile), and the ScanSetting is set to the previously defined ScanSetting (scan1).
+
image::day-to-day/scansettingbinding_details.png[link=self, window=blank, width=100%]
+
. After the ScanSettingBinding is created this will kick of the fedramp1 Compliance Suite
+
image::day-to-day/compliance_suite.png[link=self, window=blank, width=100%]
+
. This Compliance Suite (fedramp1) kicks off the defined profile (rhcos4-moderate) scans for each of the defined roles in the ScanSetting (master/worker) 
+
image::day-to-day/compliance_scan.png[link=self, window=blank, width=100%]
+
. The scan takes ~3-4 minutes to complete, then you can check the failures on the ComplianceCheckResult tab.
+
image::day-to-day/checkcomplianceresults.png[link=self, window=blank, width=100%]

== Configure Network Policies to manage VM Traffic

=== Introduction

To demo network policy, we have a standard VM.
The user logs in via console, can ping google or other public IP by default.
Show how we can set a network policy to block egress of that vm, or limit egress to only other VMs in the same namespace.

==== See that the VM can ping google

. Click VirtualMachines, click *rhel9-vm1* click Console.
If you do not see any Virtual Machines make sure you are in vms-aap-day2 project.
+
image::day-to-day/view_vm.png[link=self, window=blank, width=100%]
+
. Click on the Console tab and using the provided login perform a ping test to google.com.
+
image::day-to-day/login_vm.png[link=self, window=blank, width=100%]
+
NOTE: There is a Copy to clipboard button and a Paste button available here, which makes the login process much easier.
+
. Once you are logged in, execute the following command:
.. `ping google.com`
+
image::day-to-day/ping_site.png[link=self, window=blank, width=100%]
+
. Press *Control+C* to stop the ping.
+
. From the OpenShift console, Click on Workloads > Pods. Click on the pod name for *rhel9-vm1*.
+
image::day-to-day/select_pod.png[link=self, window=blank, width=100%]
+
. In the Pod details section click Edit.
+
image::day-to-day/pod_details.png[link=self, window=blank, width=100%]
+
. Add app=network-policy-deny to the labels and click Save.
+
image::day-to-day/pod_label.png[link=self, window=blank, width=100%]
+
. Repeat this process for *rhel9-vm2*.

==== Create the Network Policy

. Under Networking click on *NetworkPolicies* then *Create NetworkPolicy*.
+
image::day-to-day/networkpolicy.png[link=self, window=blank, width=100%]
+
. In NetworkPolicies fill out these fields and Click *Save*.

.. Policy name: ping-egress-deny
.. Key: app
.. Value: network-policy-deny
.. Check Deny all egress traffic
+
image::day-to-day/network_policy_configure.png[link=self, window=blank, width=100%]
+
. You can verify what pods are affected pods by the network policy.
+
image::day-to-day/affected_pod.png[link=self, window=blank, width=100%]

==== See that the VM CANNOT ping google

. Go back to Virtualization > *Virtual Machines* and click *rhel9-vm1*.
.. Click on the *Console* tab and login to the VM
.. Enter the command ping google.com. You should get no responce.
+
image::day-to-day/ping_site_deny.png[link=self, window=blank, width=100%]
+
. Once complete, delete the network policy you created.

== Enable and Explore Alerts, Graphs, and Logs

== VM Management (Patch the OS, and Restart the Guest)
